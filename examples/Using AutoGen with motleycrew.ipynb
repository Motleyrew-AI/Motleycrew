{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a864294",
   "metadata": {},
   "source": [
    "# Using AutoGen with motleycrew\n",
    "\n",
    "Microsoft AutoGen is one of the most popular multi-agent frameworks. Motleycrew supports interaction with autogen in both directions: firstly, you can wrap an AutoGen chat as a motleycrew tool, which you can then give to any supported agent; secondly, you can give any motleycrew tool (which includes agents used as tools) to an AutoGen agent. \n",
    "\n",
    "The two options are described in detail below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4360c4e9-db54-4ecd-a2c1-0c651e678578",
   "metadata": {},
   "source": [
    "Let's install external dependencies for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a800a778-92fc-4a37-835c-37ff07f787a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckduckgo-search==5.3.0b4\n",
      "  Using cached duckduckgo_search-5.3.0b4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: click>=8.1.7 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from duckduckgo-search==5.3.0b4) (8.1.7)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpx[brotli,http2,socks]>=0.27.0->duckduckgo-search==5.3.0b4) (0.27.2)\n",
      "Requirement already satisfied: anyio in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpx>=0.27.0->httpx[brotli,http2,socks]>=0.27.0->duckduckgo-search==5.3.0b4) (4.6.0)\n",
      "Requirement already satisfied: certifi in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpx>=0.27.0->httpx[brotli,http2,socks]>=0.27.0->duckduckgo-search==5.3.0b4) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpx>=0.27.0->httpx[brotli,http2,socks]>=0.27.0->duckduckgo-search==5.3.0b4) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpx>=0.27.0->httpx[brotli,http2,socks]>=0.27.0->duckduckgo-search==5.3.0b4) (3.10)\n",
      "Requirement already satisfied: sniffio in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpx>=0.27.0->httpx[brotli,http2,socks]>=0.27.0->duckduckgo-search==5.3.0b4) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->httpx[brotli,http2,socks]>=0.27.0->duckduckgo-search==5.3.0b4) (0.14.0)\n",
      "Requirement already satisfied: brotli in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpx[brotli,http2,socks]>=0.27.0->duckduckgo-search==5.3.0b4) (1.1.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpx[brotli,http2,socks]>=0.27.0->duckduckgo-search==5.3.0b4) (4.1.0)\n",
      "Requirement already satisfied: socksio==1.* in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpx[brotli,http2,socks]>=0.27.0->duckduckgo-search==5.3.0b4) (1.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.27.0->duckduckgo-search==5.3.0b4) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.27.0->duckduckgo-search==5.3.0b4) (4.0.0)\n",
      "Using cached duckduckgo_search-5.3.0b4-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: duckduckgo-search\n",
      "Successfully installed duckduckgo-search-5.3.0b4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting lunary==1.1.5\n",
      "  Using cached lunary-1.1.5-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.5 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from lunary==1.1.5) (3.10.8)\n",
      "Requirement already satisfied: chevron<0.15.0,>=0.14.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from lunary==1.1.5) (0.14.0)\n",
      "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.4 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from lunary==1.1.5) (3.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from lunary==1.1.5) (23.2)\n",
      "Requirement already satisfied: pyhumps<4.0.0,>=3.8.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from lunary==1.1.5) (3.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from lunary==1.1.5) (2.32.3)\n",
      "Requirement already satisfied: setuptools<73.0.0,>=72.1.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from lunary==1.1.5) (72.2.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from lunary==1.1.5) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.5->lunary==1.1.5) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.5->lunary==1.1.5) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.5->lunary==1.1.5) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.5->lunary==1.1.5) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.5->lunary==1.1.5) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.5->lunary==1.1.5) (1.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->lunary==1.1.5) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->lunary==1.1.5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->lunary==1.1.5) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->lunary==1.1.5) (2024.8.30)\n",
      "Using cached lunary-1.1.5-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: lunary\n",
      "Successfully installed lunary-1.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install duckduckgo-search==5.3.0b4\n",
    "\n",
    "# install lunary if you need logging\n",
    "%pip install lunary==1.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6602c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888adcb2",
   "metadata": {},
   "source": [
    "## Integrating an AutoGen chat into motleycrew\n",
    "\n",
    "If you want to use an AutoGen chat (say one from their many wonderful examples, or one you have already) as part of a motleycrew setup, you can wrap it as a tool, which you can then give to any other motleycrew-supported agent.\n",
    "\n",
    "Let's create an AutoGen chat for automatic problem solving.\n",
    "The code is taken from the example here: https://microsoft.github.io/autogen/docs/notebooks/agentchat_groupchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be73aa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyautogen in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (0.3.1)\n",
      "Requirement already satisfied: diskcache in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from pyautogen) (5.6.3)\n",
      "Requirement already satisfied: docker in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from pyautogen) (7.1.0)\n",
      "Requirement already satisfied: flaml in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from pyautogen) (2.3.1)\n",
      "Requirement already satisfied: numpy<2,>=1.17.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from pyautogen) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.3 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from pyautogen) (1.50.2)\n",
      "Requirement already satisfied: packaging in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from pyautogen) (23.2)\n",
      "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from pyautogen) (2.9.2)\n",
      "Requirement already satisfied: python-dotenv in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from pyautogen) (1.0.1)\n",
      "Requirement already satisfied: termcolor in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from pyautogen) (2.5.0)\n",
      "Requirement already satisfied: tiktoken in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from pyautogen) (0.7.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from openai>=1.3->pyautogen) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from openai>=1.3->pyautogen) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from openai>=1.3->pyautogen) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from openai>=1.3->pyautogen) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from openai>=1.3->pyautogen) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from openai>=1.3->pyautogen) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from openai>=1.3->pyautogen) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (2.23.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from docker->pyautogen) (2.32.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from docker->pyautogen) (2.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from tiktoken->pyautogen) (2024.9.11)\n",
      "Requirement already satisfied: idna>=2.8 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (3.10)\n",
      "Requirement already satisfied: certifi in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from requests>=2.26.0->docker->pyautogen) (3.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyautogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3da0bc-d0f6-4f9c-8e69-0ad126f3a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/stefan/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "import os\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}],\n",
    "    \"cache_seed\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f7738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 2,\n",
    "        \"work_dir\": \"examples/data/groupchat\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    human_input_mode=\"TERMINATE\",\n",
    ")\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in software product ideas.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=12)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8480361",
   "metadata": {},
   "source": [
    "If we were using plain AutoGen, we'd start the chat with something like this:\n",
    "```\n",
    "user_proxy.initiate_chat(\n",
    "    manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"\n",
    ")\n",
    "```\n",
    "\n",
    "You see, the chat accepts an input and returns an output, just like a tool.  \n",
    "Because of that, for using the chat in motleycrew, we can utilize the built-in AutoGenChatTool. Its prompt can either be a template or a plain string. Here we are creating a tool that searches arXiv for recent papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d610dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from motleycrew.tools.autogen_chat_tool import AutoGenChatTool\n",
    "\n",
    "knowledge_retrieval_tool = AutoGenChatTool(\n",
    "    name=\"retrieve_knowledge_by_topic\",\n",
    "    description=\"Search arxiv for the latest paper on a given topic \"\n",
    "                \"and find its potential applications in software.\",  # will be used in the prompt of the future agent that will use the tool\n",
    "    prompt=\"Find a latest paper about {topic} on arxiv \"\n",
    "            \"and find its potential applications in software.\",  # this is the initial prompt for the AutoGen chat itself\n",
    "    initiator=user_proxy,\n",
    "    recipient=manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580e262",
   "metadata": {},
   "source": [
    "We can now give the tool to any agent and solve tasks with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "534453a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from motleycrew import MotleyCrew\n",
    "from motleycrew.agents.langchain import ReActToolCallingMotleyAgent\n",
    "\n",
    "crew = MotleyCrew()\n",
    "writer = ReActToolCallingMotleyAgent(tools=[knowledge_retrieval_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d9d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from motleycrew.tasks import SimpleTask\n",
    "\n",
    "blog_post_task = SimpleTask(\n",
    "        crew = crew,\n",
    "        name=\"Produce blog post on the applications of latest advancements related to GPT-4\",\n",
    "        description=\"Using the insights provided by searching research papers, develop an engaging blog \"\n",
    "                    \"post that highlights the most significant advancements on GPT-4 ant their applications.\\n\"\n",
    "                    \"Your post should be informative yet accessible, catering to a tech-savvy audience.\\n\"\n",
    "                    \"Make it sound cool, avoid complex words so it doesn't sound like AI. \"\n",
    "                    \"Create a blog post of at least 4 paragraphs.\",\n",
    "        agent=writer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf0c1a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about GPT-4 advancements on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "To fetch the latest paper about GPT-4 advancements from arXiv and explore its potential applications in software, the following python script uses the `arxiv` library that makes it easy to query the arXiv API, find the relevant papers about GPT-4, and display the title, summary, and categories of the latest paper. This will help us understand what kind of advancements have been made and the potential domains of application.\n",
      "\n",
      "Here’s the python script that you will need to run:\n",
      "\n",
      "```python\n",
      "# filename: fetch_latest_gpt4_paper.py\n",
      "import arxiv\n",
      "\n",
      "# Search for papers on arXiv about GPT-4\n",
      "search = arxiv.Search(\n",
      "  query=\"GPT-4\",\n",
      "  max_results=1,\n",
      "  sort_by=arxiv.SortCriterion.SubmittedDate,  # Fetches the most recent submission\n",
      "  sort_order=arxiv.SortOrder.Descending\n",
      ")\n",
      "\n",
      "# Fetch and display the information for the latest paper\n",
      "for result in search.results():\n",
      "    print(\"Title:\", result.title)\n",
      "    print(\"Authors:\", \", \".join(author.name for author in result.authors))\n",
      "    print(\"Summary:\", result.summary.replace('\\n', ' '))\n",
      "    print(\"Categories:\", \", \".join(category for category in result.categories))\n",
      "    print(\"Published Date:\", result.published)\n",
      "    print(\"PDF Link:\", result.pdf_url)\n",
      "```\n",
      "\n",
      "To proceed:\n",
      "1. Ensure you have the `arxiv` Python library installed. If not, you can install it using pip:\n",
      "\n",
      "   ```\n",
      "   pip install arxiv\n",
      "   ```\n",
      "\n",
      "2. Save the above code into a file named `fetch_latest_gpt4_paper.py`.\n",
      "3. Run the script using Python:\n",
      "\n",
      "   ```\n",
      "   python fetch_latest_gpt4_paper.py\n",
      "   ```\n",
      "\n",
      "The script will display the latest GPT-4 paper it finds on arXiv. After you run this and provide me with the summary from the output, I can then help identify the potential software applications described in the paper.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"fetch_latest_gpt4_paper.py\", line 2, in <module>\n",
      "    import arxiv\n",
      "ModuleNotFoundError: No module named 'arxiv'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"fetch_latest_gpt4_paper.py\", line 2, in <module>\n",
      "    import arxiv\n",
      "ModuleNotFoundError: No module named 'arxiv'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "It appears that the ‘arxiv’ library required for the script has not been installed. Please install the library using the following command and then execute the script again:\n",
      "\n",
      "```sh\n",
      "pip install arxiv\n",
      "```\n",
      "\n",
      "After installing the library, rerun the script `fetch_latest_gpt4_paper.py` by issuing the following command:\n",
      "\n",
      "```sh\n",
      "python fetch_latest_gpt4_paper.py\n",
      "```\n",
      "\n",
      "This should fetch the latest GPT-4 paper from arXiv. Please provide the output afterwards so I can help identify the potential applications in software from the paper's content.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Using cached sgmllib3k-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/stefan/.cache/pypoetry/virtualenvs/motleycrew-RnW5x4dY-py3.12/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n",
      "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "\n",
      "Title: SWE-Bench+: Enhanced Coding Benchmark for LLMs\n",
      "Authors: Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, Song Wang\n",
      "Summary: Large Language Models (LLMs) in Software Engineering (SE) can offer assistance for coding. To facilitate a rigorous evaluation of LLMs in practical coding contexts, Carlos et al. introduced the SWE-bench dataset, which comprises 2,294 real-world GitHub issues and their corresponding pull requests, collected from 12 widely used Python repositories. Several impressive LLM-based toolkits recently are developed and evaluated on this dataset. However, a systematic evaluation of the quality of SWE-bench remains missing. In this paper, we addressed this gap by presenting an empirical analysis of the SWE-bench dataset. We conducted a manual screening of instances where SWEAgent + GPT-4 successfully resolved issues by comparing the model-generated patches with the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench leaderboard during the time of our study. Our analysis reveals some critical issues with the SWE-bench dataset: 1) 32.67% of the successful patches involve cheating as the solutions were directly provided in the issue report or the comments. We refer to as solution leakage problem. 2) 31.08% of the passed patches are suspicious patches due to weak test cases, i.e., the tests were not adequate to verify the correctness of a patch. When we filtered out these problematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47% to 3.97%. We also observed that the same data quality issues also exist in the two variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In addition, over 94% of the issues were created before LLM's knowledge cutoff dates, posing potential data leakage issues.\n",
      "Categories: cs.SE\n",
      "Published Date: 2024-10-09 15:38:53+00:00\n",
      "PDF Link: http://arxiv.org/pdf/2410.06992v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "The latest paper retrieved from arXiv titled **\"SWE-Bench+: Enhanced Coding Benchmark for LLMs\"** discusses advancements related to Large Language Models (LLMs) like GPT-4 specifically in the context of software engineering. Here are some important insights regarding its potential applications in software:\n",
      "\n",
      "1. **Automated Code Generation**: The SWE-Agent+GPT-4 demonstrated the ability to resolve issues by generating code patches based on real-world GitHub issues. This application shows promise for automated pull requests and bug fixes.\n",
      "\n",
      "2. **Evaluation of LLMs in Real-world Scenarios**: The paper emphasizes rigorous evaluation of LLMs in practical coding contexts, implying that GPT-4 can significantly contribute to ensuring the reliability and quality of automations in software development.\n",
      "\n",
      "3. **Quality Control in Machine-Generated Code**: The paper addresses issues like 'solution leakage' and weak test cases impacting the performance of GPT-4. The analysis in the paper could help improve the mechanisms for validating and verifying machine-generated code, making it safe and reliable for practical use.\n",
      "\n",
      "The analysis also highlighted the importance of data quality in training these models, where data leakage and inadequate tests can mislead the effectiveness of the model. This understanding can drive the development of better training datasets and evaluation protocols for AI-driven development tools.\n",
      "\n",
      "Overall, the potential applications of GPT-4 in software outlined by this academic work include enhancing automated coding tools, improving software development testing, and refining dataset quality for training AI models. These advancements can lead to more robust, AI-assisted programming environments, potentially reducing human error and increasing development efficiency.\n",
      "\n",
      "This reflects a substantial forward in how AI can assist in software engineering, hinting at broader implementation and deeper integration of LLMs into the software development lifecycle.\n",
      "\n",
      "If you find these findings intriguing and wish to delve deeper into the details of the research, you can further explore the study via the provided PDF link: [SWE-Bench+: Enhanced Coding Benchmark for LLMs PDF](http://arxiv.org/pdf/2410.06992v1)\n",
      "\n",
      "Would there be anything specific you'd like to explore or inquire further about this topic?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "It appears there was a misunderstanding. If you have specific software product ideas in relation to GPT-4 advancements or any other requests, please let me know so I can assist you further.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "It looks like there was a bit of confusion in our last exchange. If you have any specific queries or directions regarding using GPT-4 advancements in software product ideas or any other questions, please let me know! I'm here to help with more tailored responses or to explore other areas of interest you might have.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about GPT-4 advancements on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Coder\n",
      "\u001b[0m\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "First, let's search for the latest papers related to GPT-4 advancements on arXiv.org using their API. To do this, we will query the arXiv API for papers mentioning \"GPT-4\" and sort them by submission date to get the most recent publications.\n",
      "\n",
      "```python\n",
      "import urllib.request\n",
      "import urllib.parse\n",
      "import feedparser\n",
      "\n",
      "# Base URL for the arXiv API\n",
      "base_url = 'http://export.arxiv.org/api/query?'\n",
      "\n",
      "# Search parameters\n",
      "query = 'ti:\"GPT-4\"'\n",
      "params = {\n",
      "    'search_query': query,\n",
      "    'sortBy': 'submittedDate',\n",
      "    'sortOrder': 'descending',\n",
      "    'max_results': 1\n",
      "}\n",
      "\n",
      "# Form the request URL\n",
      "url = base_url + urllib.parse.urlencode(params)\n",
      "\n",
      "# Perform the request\n",
      "response = urllib.request.urlopen(url)\n",
      "feed = feedparser.parse(response)\n",
      "\n",
      "# Check if any results were found\n",
      "if len(feed.entries) > 0:\n",
      "    title = feed.entries[0].title\n",
      "    summary = feed.entries[0].summary\n",
      "    published = feed.entries[0].published\n",
      "    link = feed.entries[0].links[0].href\n",
      "\n",
      "    print(f\"Title: {title}\\nPublished: {published}\\nLink: {link}\\nSummary: {summary}\")\n",
      "else:\n",
      "    print(\"No results found for GPT-4 on arXiv.\")\n",
      "```\n",
      "\n",
      "Please run the above Python script. It will find and print details about the latest paper related to GPT-4 on arXiv, including the title, publication date, link, and a brief summary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: Creative and Context-Aware Translation of East Asian Idioms with GPT-4\n",
      "Published: 2024-10-01T18:24:43Z\n",
      "Link: http://arxiv.org/abs/2410.00988v1\n",
      "Summary: As a type of figurative language, an East Asian idiom condenses rich cultural\n",
      "background into only a few characters. Translating such idioms is challenging\n",
      "for human translators, who often resort to choosing a context-aware translation\n",
      "from an existing list of candidates. However, compiling a dictionary of\n",
      "candidate translations demands much time and creativity even for expert\n",
      "translators. To alleviate such burden, we evaluate if GPT-4 can help generate\n",
      "high-quality translations. Based on automatic evaluations of faithfulness and\n",
      "creativity, we first identify Pareto-optimal prompting strategies that can\n",
      "outperform translation engines from Google and DeepL. Then, at a low cost, our\n",
      "context-aware translations can achieve far more high-quality translations per\n",
      "idiom than the human baseline. We open-source all code and data to facilitate\n",
      "further research.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "The latest paper on GPT-4, titled \"Creative and Context-Aware Translation of East Asian Idioms with GPT-4,\" describes an innovative application of GPT-4 in language translation, particularly focusing on translating complex East Asian idioms. The paper is available [here](http://arxiv.org/abs/2410.00988v1).\n",
      "\n",
      "**Potential Applications in Software:**\n",
      "\n",
      "1. **Translation Software Enhancement:**\n",
      "   - Implementing GPT-4 in translation software could significantly improve the translation quality of idiomatic phrases, which are often a challenge due to their contextual and cultural depth. Software could provide multiple translations with contextual nuances, improving user understanding and satisfaction.\n",
      "\n",
      "2. **Language Learning Tools:**\n",
      "   - Language learning platforms can integrate GPT-4 to offer more nuanced translations of phrases and idioms, helping learners understand the cultural context better. This could enhance comprehension and retention for students studying East Asian languages.\n",
      "\n",
      "3. **Content Localization Services:**\n",
      "   - Companies that require localization of content into multiple languages, such as media services or multinational corporations, could use GPT-4-enhanced tools to ensure that the translated content retains the intended meaning and cultural significance.\n",
      "\n",
      "4. **Dictionary and Reference Applications:**\n",
      "   - Digital dictionaries and reference applications can leverage GPT-4 to provide users with rich, contextually appropriate interpretations of idioms or phrases, rather than static, literal translations.\n",
      "\n",
      "5. **Creative Writing Software:**\n",
      "   - Tools designed to aid in creative writing or content generation could incorporate GPT-4 to suggest creative ways to use idiomatic expressions in various languages, enhancing the narrative quality and engagement of written content.\n",
      "\n",
      "6. **Automated Subtitling and Dubbing in Media Production:**\n",
      "   - Media production companies can apply GPT-4-based translation frameworks to automate and improve the quality of subtitling and dubbing in different languages, particularly for content rich in cultural idioms.\n",
      "\n",
      "This advancement offers an exciting glimpse into how AI can bridge the gap between languages and cultures, enhancing understanding and communication globally.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "This research demonstrates GPT-4's potential in language translation applications, specifically for translating complex East Asian idioms. Here are possible applications of this technology in software:\n",
      "\n",
      "1. **Advanced Translation Services**: GPT-4 can be integrated into translation software to provide more accurate and culturally relevant translations of idioms. This would be particularly useful in professional settings where precise translation can avoid miscommunication.\n",
      "\n",
      "2. **Educational Tools**: Educational platforms could use GPT-4 to help students understand idioms and expressions from different cultures in their original context, improving language learning by providing insights into cultural nuances.\n",
      "\n",
      "3. **Content Localization**: Companies looking to expand globally often need to adapt their content to local cultures. GPT-4's ability to provide context-aware translations could improve content localization, making products more accessible and appealing to international markets.\n",
      "\n",
      "4. **Customer Support**: AI-powered chatbots and customer support tools could use GPT-4 to better understand and respond to queries that include idiomatic expressions, providing a more effective and satisfying user experience for customers speaking different languages.\n",
      "\n",
      "5. **Creative Writing**: Software tools designed to assist with writing could incorporate GPT-4 to suggest or decode idiomatic expressions, aiding writers in creating more authentic and engaging content for diverse audiences.\n",
      "\n",
      "These applications could significantly enhance the capabilities of software products by providing deeper linguistic and cultural understanding, essential for global interaction in the digital age.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Given the significant potential of GPT-4 in enhancing translation accuracy and contextual understanding, especially for complex idiomatic expressions in East Asian languages, here are various software product ideas that could leverage such advancements:\n",
      "\n",
      "### Multilingual Customer Support Chatbots\n",
      "1. **Improved Understanding**: GPT-4 could enable chatbots to understand and employ idiomatic language effectively, providing more natural and relatable interactions for users who communicate using these expressions.\n",
      "2. **Cultural Sensitivity**: Chatbots could be trained to recognize cultural nuances, improving engagement with users from diverse backgrounds.\n",
      "\n",
      "### Translation Tools for Authors and Creators\n",
      "1. **Cultural Translation Features**: Tools that help authors translate their works while keeping cultural meanings intact, preserving the original sentiment and context.\n",
      "2. **Assisted Writing Modes**: For writers looking to authentically incorporate foreign idioms into their work, offering suggestions and translations that retain the flavor of the source language.\n",
      "\n",
      "### Enhanced Subtitling Applications for Media\n",
      "1. **Automated Contextual Subtitling**: Software that uses GPT-4 to generate subtitles not just for direct translation but with contextual adjustments ensuring cultural relevance and comprehension for diverse audiences.\n",
      "2. **Creative Subtitling**: Tools for subtitlers that suggest multiple translation options per idiom, based on varying levels of formality or directness, enhancing viewer engagement and satisfaction.\n",
      "\n",
      "### Educational Language Learning Platforms\n",
      "1. **Contextual Learning Modules**: Use GPT-4 to design language learning curricula that focus on the use of idioms in everyday conversation, providing learners with the cultural context behind the expressions.\n",
      "2. **Interactive Scenarios**: Develop interactive scenarios where learners can see idioms used in different contexts, helping to solidify understanding and usage.\n",
      "\n",
      "### Localized Content Generation for Marketers\n",
      "1. **Automated Content Localization**: Tools that automatically adapt content (including idioms and colloquialisms) to fit local markets more effectively, enhancing user engagement and brand relatability.\n",
      "2. **Marketing Aide for Cultural Nuances**: Software that suggests modifications or enhancements to marketing materials to make them more appealing to specific regional audiences.\n",
      "\n",
      "These product ideas not only focus on translating language but also emphasize understanding and conveying underlying cultural contexts, which could dramatically improve communication and user experience across various digital platforms.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TaskUnit(status=done)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crew.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ae5789a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Final Answer: \n",
       "\n",
       "**Unveiling the Magic of GPT-4: Transforming Language and Culture**\n",
       "\n",
       "In the ever-evolving world of artificial intelligence, GPT-4 stands out as a beacon of innovation, pushing the boundaries of what's possible in language processing. Imagine a world where language barriers crumble, and cultural nuances are effortlessly understood. That's the promise of GPT-4, especially when it comes to translating complex idiomatic expressions in East Asian languages. This isn't just about words; it's about capturing the essence of culture and context, making communication more authentic and relatable.\n",
       "\n",
       "One of the coolest applications of GPT-4 is in the realm of multilingual customer support chatbots. These aren't your average bots; they're culturally savvy, understanding and employing idiomatic language to create natural interactions. Imagine chatting with a bot that gets your cultural references and responds in a way that feels genuinely human. It's like having a conversation with a local, no matter where you are in the world. This level of cultural sensitivity can revolutionize customer engagement, making interactions more meaningful and effective.\n",
       "\n",
       "For authors and creators, GPT-4 opens up a world of possibilities in translation tools. Picture a tool that not only translates your work but also preserves the cultural meanings and sentiments behind it. Whether you're an author looking to share your story with a global audience or a writer wanting to incorporate foreign idioms authentically, GPT-4 has got you covered. It's like having a cultural ambassador by your side, ensuring your message resonates with readers from different backgrounds.\n",
       "\n",
       "The media industry, too, can benefit from GPT-4's advancements. Automated contextual subtitling is a game-changer, generating subtitles that go beyond direct translation to ensure cultural relevance and comprehension. Imagine watching a foreign film where the subtitles capture not just the dialogue but the cultural nuances, enhancing your viewing experience. It's like having a personal translator who understands the subtleties of language and culture, making global content more accessible and enjoyable.\n",
       "\n",
       "In the world of education, GPT-4 is a powerful ally for language learners. With contextual learning modules, students can dive into the world of idioms, understanding their use in everyday conversation. Interactive scenarios bring these expressions to life, helping learners grasp their meaning and usage in different contexts. It's like having a language coach who not only teaches you the words but also the stories behind them, enriching your learning journey.\n",
       "\n",
       "GPT-4 is not just about language; it's about bridging cultures and enhancing communication. Whether it's through chatbots, translation tools, media applications, or educational platforms, the potential is limitless. As we continue to explore and harness the power of GPT-4, one thing is clear: the future of language processing is here, and it's more exciting than ever."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(blog_post_task.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c5eb1",
   "metadata": {},
   "source": [
    "The writer agent used the AutoGen chat tool to retrieve data for the post!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf1868-9567-4240-b7e3-d5f6a50d3974",
   "metadata": {},
   "source": [
    "## Using any motleycrew-supported tools with Autogen agents\n",
    "\n",
    "Now let's do it the other way around: give tools (or agents, for that matter) from motleycrew to AutoGen agents. The code is based on https://microsoft.github.io/autogen/docs/tutorial/tool-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a79da460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function motleycrew.tools.tool.MotleyTool.to_autogen_tool.<locals>.autogen_tool_fn(input: str) -> str>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from motleycrew.tools import MotleyTool, RetryConfig\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search_tool = MotleyTool.from_supported_tool(\n",
    "    DuckDuckGoSearchRun(),\n",
    "    retry_config=RetryConfig(max_retries=5)  # for retrying rate limit errors\n",
    ")  # Any tools or even motleycrew's agents can be converted to MotleyTool like this!\n",
    "\n",
    "# Let's first define the assistant agent that suggests tool calls.\n",
    "assistant = autogen.ConversableAgent(\n",
    "    name=\"Assistant\",\n",
    "    system_message=\"You are a helpful AI assistant. \"\n",
    "    \"You can provide useful and up-to-date data using web search. \"\n",
    "    \"Return 'TERMINATE' when the task is done.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# The user proxy agent is used for interacting with the assistant agent\n",
    "# and executes tool calls.\n",
    "user_proxy = autogen.ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Make an autogen-compatible tool from the MotleyTool.\n",
    "autogen_tool = search_tool.to_autogen_tool()\n",
    "\n",
    "# Register the tool signature with the assistant agent.\n",
    "assistant.register_for_llm(name=\"search_tool\", description=\"Web search tool\")(autogen_tool)\n",
    "\n",
    "# Register the tool function with the user proxy agent.\n",
    "user_proxy.register_for_execution(name=\"search_tool\")(autogen_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6027c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "What was the first computer?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_lp7DaZO1WUlu7YsMZazwsZX2): search_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\"input\":\"first computer\"}\n",
      "\u001b[32m****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION search_tool...\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_lp7DaZO1WUlu7YsMZazwsZX2) *****\u001b[0m\n",
      "Learn how the English mathematician and inventor Charles Babbage designed the Difference Engine and the Analytical Engine, the first mechanical digital computers. Explore their features, components, and challenges in this article from Britannica. ENIAC was built by the United States during World War II and completed in 1946. It was the first general-purpose digital computer that could execute different instructions based on data values. Learn about the history of computing, from Charles Babbage's mechanical difference and analytical engines to the first electronic computer, the Atanasoff-Berry Computer. Discover how World War II and personal computers shaped the evolution of computers. Computer - ENIAC, Electronic, Computing: In the United States, government funding went to a project led by John Mauchly, J. Presper Eckert, Jr., and their colleagues at the Moore School of Electrical Engineering at the University of Pennsylvania; their objective was an all-electronic computer. Under contract to the army and under the direction of Herman Goldstine, work began in early 1943 on ... Learn about 10 of the earliest devices that shaped the concepts of computation and automated calculation, from ancient Greek gears to modern electronic circuits. Discover how they were used for astronomy, cryptography, physics, and more.\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "The title of \"first computer\" can be attributed to different machines depending on the criteria used (mechanical vs. digital, programmable vs. non-programmable).\n",
      "\n",
      "1. **Difference Engine and Analytical Engine by Charles Babbage**:\n",
      "   - Charles Babbage, an English mathematician and inventor, designed the Difference Engine and the Analytical Engine in the 19th century. These were mechanical digital computers meant to automate the process of computing tables and had components analogous to a CPU, memory, and input/output paths.\n",
      "   - Although the full versions of these engines were never completed during Babbage’s lifetime, they are often considered the first designs for a programmable computer.\n",
      "\n",
      "2. **Atanasoff-Berry Computer (ABC)**:\n",
      "   - Conceived in the 1930s by John Atanasoff and Clifford Berry, the ABC was the first electronic digital computer. It was designed to solve systems of linear equations and was not Turing-complete nor programmable in the modern sense.\n",
      "\n",
      "3. **ENIAC (Electronic Numerical Integrator and Computer)**:\n",
      "   - Developed by John Mauchly and J. Presper Eckert during World War II, and completed in 1946. ENIAC was considered the first high-speed, Turing-complete, digital computer capable of being reprogrammed to solve a full range of computing problems. \n",
      "\n",
      "The ENIAC often gets credited as the first \"general-purpose digital computer,\" given its electronic and programmable nature, distinguishing it from earlier mechanical or less flexible designs.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "If you have any more questions or need further information about historical computers or any other topic, feel free to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_result = user_proxy.initiate_chat(assistant, message=\"What was the first computer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d4b7df4-7327-46fe-81be-4e4e580638c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent used the search tool!\n"
     ]
    }
   ],
   "source": [
    "final_result = 'The agent used the search tool!'\n",
    "print(final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
